{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Fraud Detection\n",
    "\n",
    "In this project you will predict fraudulent credit card transactions with the help of Machine learning models. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from helper import get_detailed_info\n",
    "from helper import get_numeric_metadata\n",
    "from helper import get_frequency_distribution\n",
    "from helper import get_group_frequency_dist\n",
    "from helper import plot_box_bar_or_scatter_plot\n",
    "from helper import plot_dist_plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "\n",
    "\n",
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for Normality in Python\n",
    "# This function will return the skewness, kurtosis and whether the data is normally distributed or not \n",
    "# for the numerical features\n",
    "# Just pass the dataframe and it will extract the numeric features to perform normality test on the features\n",
    "# Returns dataframe\n",
    "# This method is based upon the DataCamp tutorial on Portfolio Risk management\n",
    "\n",
    "# https://www.spcforexcel.com/knowledge/basic-statistics/are-skewness-and-kurtosis-useful-statistics#:~:text=So%2C%20a%20normal%20distribution%20will,sizes%20of%20the%20two%20tails.&text=If%20the%20kurtosis%20is%20greater,(more%20in%20the%20tails).\n",
    "\n",
    "# https://campus.datacamp.com/courses/introduction-to-portfolio-risk-management-in-python/univariate-investment-risk-and-returns?ex=10\n",
    "def test_for_normality(df):\n",
    "    \"\"\"Testing for Normality in Python\n",
    "       This function will return the skewness, kurtosis and whether the data is normally distributed or not for the numerical features\n",
    "\n",
    "    Arguments:\n",
    "        df {[DataFrame]} -- Pass the pandas dataframe (with numeric features only)\n",
    "\n",
    "    Returns:\n",
    "        [DataFrame] -- The dataframe\n",
    "    \"\"\"\n",
    "    ref_df = df.copy()\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[\"Features\"] = ref_df.columns\n",
    "\n",
    "    skew = \"skew\"\n",
    "    kurtosis = \"kurtosis\"\n",
    "    isNormal = \"isNormal\"\n",
    "\n",
    "    for col in ref_df.columns.values:\n",
    "        # get index\n",
    "        idx = result_df[result_df[\"Features\"] == col].index[0]\n",
    "\n",
    "        # Skew\n",
    "        result_df.loc[idx, skew] = ref_df[col].skew()\n",
    "        result_df.loc[idx, kurtosis] = ref_df[col].kurtosis()\n",
    "\n",
    "        # Shapiro-Wilk test\n",
    "        # The null hypothesis of the Shapiro-Wilk test assumes that the data are normally distributed.\n",
    "        # If the p-value is less than 0.05, the null hypothesis is rejected because the data are most likely non-normal.\n",
    "        p_value = stats.shapiro(ref_df[col].dropna())[1]\n",
    "        if p_value <= 0.05:\n",
    "            result_df.loc[idx, isNormal] = False\n",
    "        else:\n",
    "            result_df.loc[idx, isNormal] = True\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def get_higly_correlated_columns_name(row, positive=0.8, negative=-0.8):\n",
    "    corr_col_arr = []\n",
    "    for col in row.index.values:\n",
    "        if (row[col] >= positive or row[col] <= negative):\n",
    "            corr_col_arr.append(col)\n",
    "    return corr_col_arr\n",
    "\n",
    "\n",
    "def get_higly_correlated_columns_value(row, positive=0.8, negative=-0.8):\n",
    "    corr_col_val_arr = []\n",
    "    for col in row.index.values:\n",
    "        if ((row[col] >= positive ) or (row[col] <= negative)):\n",
    "            corr_col_val_arr.append(str(round(row[col], 2)))\n",
    "    return corr_col_val_arr\n",
    "\n",
    "\n",
    "def get_numeric_correlation(df, thresh=0.8):\n",
    "\n",
    "    corr_matrix = df.corr()\n",
    "    \n",
    "    cor = corr_matrix.copy()\n",
    "    \n",
    "    cor[\"cor_columns\"] = cor.apply(\n",
    "        lambda x: get_higly_correlated_columns_name(x, thresh, -(thresh)), axis=1\n",
    "    )\n",
    "    cor[\"cor_columns_val\"] = cor.iloc[:, :-1].apply(\n",
    "        lambda x: get_higly_correlated_columns_value(x, thresh, -(thresh)), axis=1\n",
    "    )\n",
    "\n",
    "    cor = cor.iloc[:, -2:]\n",
    "    return cor, corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get detailed summary report\n",
    "get_detailed_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, from above detail report of dataset, we have 30 features all numeric, no missing data, no null values, no column with all unique values, we get few duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of duplicate rows\n",
    "df.loc[df.duplicated()].shape[0] / df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the fraction of duplicate rows is too low in comparison to amount of data we have, also as we have numeric data hence duplication result even for slight changes in values, hence we could ignore duplication here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get metadata info for dataset\n",
    "#observe the different feature type present in the data\n",
    "get_numeric_metadata(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will observe the distribution of our classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\n",
    "class_freq_dist = get_frequency_distribution(df, 'Class', normalize=False)\n",
    "class_freq_dist.iloc[0,0] = 'normal_share'\n",
    "class_freq_dist.iloc[1,0] = 'fraud_share'\n",
    "\n",
    "plot_box_bar_or_scatter_plot(x='Class',\n",
    "                     y='normalized_frequency_distribution',\n",
    "                     data=class_freq_dist, \n",
    "                     xlabel='Class',\n",
    "                     ylabel='Class Distribution (Count)',\n",
    "                     title='Class Frequency Distribution Count',\n",
    "                     plot=1, \n",
    "                     figsize=(10,7),\n",
    "                     ticksFont_size=14,\n",
    "                     normalize=False)\n",
    "\n",
    "class_freq_dist = get_frequency_distribution(df, 'Class', normalize=True)\n",
    "class_freq_dist.iloc[0,0] = 'normal_share'\n",
    "class_freq_dist.iloc[1,0] = 'fraud_share'\n",
    "\n",
    "plot_box_bar_or_scatter_plot(x='Class',\n",
    "                     y='normalized_frequency_distribution',\n",
    "                     data=class_freq_dist, \n",
    "                     xlabel='Class',\n",
    "                     ylabel='Class Distribution (in %)',\n",
    "                     title='Class Frequency Distribution',\n",
    "                     plot=1, \n",
    "                     figsize=(10,7),\n",
    "                     ticksFont_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dist plot to observe the distribution of classes with Amount\n",
    "plot_dist_plot(df.loc[df['Class']==0, 'Amount'],figsize=(10,6),color='g')\n",
    "\n",
    "plot_dist_plot(df.loc[df['Class']==1, 'Amount'],figsize=(10,6), color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dist plot to observe the distribution of classes with Time\n",
    "plot_dist_plot(df.loc[df['Class']==0, 'Time'],figsize=(10,6),color='g')\n",
    "\n",
    "plot_dist_plot(df.loc[df['Class']==1, 'Time'],figsize=(10,6), color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot to observe the distribution of classes with time\n",
    "sns.catplot(x=\"Class\", y=\"Time\", data=df)\n",
    "\n",
    "# Create a scatter plot to observe the distribution of classes with amount\n",
    "sns.catplot(x=\"Class\", y=\"Amount\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between features & between target and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the heatmap depicting the numeric correlations\n",
    "mapping, corr_matrix = get_numeric_correlation(df, .3)\n",
    "#corr_matrix.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the mapping of features and target to depict to correlations with the minimum threshold of 30%\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mapping.loc['Class',:][0])\n",
    "print(mapping.loc['Class',:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mapping.loc['Amount',:][0])\n",
    "print(mapping.loc['Amount',:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mapping.loc['Time',:][0])\n",
    "print(mapping.loc['Time',:][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train & test data\n",
    "- We will use stratified split to maintain the class proportion in train & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= df['Class']\n",
    "X = df.drop(['Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state=123, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preserve X_test & y_test to evaluate on the test data once you build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(y))\n",
    "print(np.sum(y_train))\n",
    "print(np.sum(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the distribution of a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 80))\n",
    "for k, col in enumerate(X_train):\n",
    "  plt.subplot(16,2,k+1)\n",
    "  sns.distplot(X_train.loc[df['Class']==0, col], color='g')\n",
    "  sns.distplot(X_train.loc[df['Class']==1, col], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 80))\n",
    "for k, col in enumerate(X_test):\n",
    "  plt.subplot(16,2,k+1)\n",
    "  sns.distplot(X_test.loc[df['Class']==0, col], color='g')\n",
    "  sns.distplot(X_test.loc[df['Class']==1, col], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there is skewness present in the distribution use:\n",
    "- <b>Power Transformer</b> package present in the <b>preprocessing library provided by sklearn</b> to make distribution more gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shapiro-Wilk test for checking normality of feature variable in train set\n",
    "test_for_normality(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shapiro-Wilk test for checking normality of feature variable in test set\n",
    "test_for_normality(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see from above that all the feature variables are non-normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Apply : preprocessing.PowerTransformer(copy=False) to fit & transform the train & test data\n",
    "tx = PowerTransformer()\n",
    "tx.fit(X_train)                      \n",
    "X_train = pd.DataFrame(tx.transform(X_train), columns=X.columns)\n",
    "X_test = pd.DataFrame(tx.transform(X_test), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shapiro-Wilk test for checking normality of feature variable in train set\n",
    "test_for_normality(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shapiro-Wilk test for checking normality of feature variable in test set\n",
    "test_for_normality(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of features from the train set to see the result \n",
    "plt.figure(figsize=(15, 80))\n",
    "for k, col in enumerate(X_train):\n",
    "  plt.subplot(16,2,k+1)\n",
    "  sns.distplot(X_train.loc[df['Class']==0, col], color='g')\n",
    "  sns.distplot(X_train.loc[df['Class']==1, col], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of features from the test set to see the result \n",
    "plt.figure(figsize=(15, 80))\n",
    "for k, col in enumerate(X_test):\n",
    "  plt.subplot(16,2,k+1)\n",
    "  sns.distplot(X_test.loc[df['Class']==0, col], color='g')\n",
    "  sns.distplot(X_test.loc[df['Class']==1, col], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "- Build different models on the imbalanced dataset and see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_validation(model, X_train, y_train, cv):\n",
    "    print('Accuracy -> (TP+TN)/total:' ,np.mean(cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)))\n",
    "    print('ROC_AUC Score :' ,np.mean(cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv)))\n",
    "    print('F1 Score -> 2*(precision*recall)/(precision+recall):' ,np.mean(cross_val_score(model, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)))\n",
    "    print('Precision -> TP/(TP+FP):' ,np.mean(cross_val_score(model, X_train, y_train, scoring='precision', cv=cv, n_jobs=-1)))\n",
    "    print('Recall -> TP/(TP+FN):' ,np.mean(cross_val_score(model, X_train, y_train, scoring='recall', cv=cv, n_jobs=-1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def TuneLRModel(X_train, y_train, k=5):\n",
    "    \n",
    "    # Instantiate logistic regression \n",
    "    LR = LogisticRegression(random_state=123)\n",
    "  \n",
    "    # Create regularization penalty space\n",
    "    C = np.logspace(-4,4,50)\n",
    "    \n",
    "    # Stratified kfold as cross validation strategy\n",
    "    cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Create params options\n",
    "    param_grid = [\n",
    "      {'C' :C, 'penalty': ['l1'], 'solver': [ 'saga', 'liblinear']},\n",
    "      {'C':C, 'penalty': ['l2'], 'solver': ['newton-cg', 'lbfgs']},\n",
    "    ]\n",
    "\n",
    "    # Use Multi scorer\n",
    "    scoring  = {'Accuracy':'accuracy','Precision': 'precision', 'Recall': 'recall', 'F1' : 'f1', 'AUC':'roc_auc'}\n",
    "   \n",
    "    # Create grid search using StratifiedKFold cross validation\n",
    "    lrSearch = GridSearchCV(LR, \n",
    "                   param_grid = param_grid, \n",
    "                   cv=cv, \n",
    "                   scoring=scoring ,\n",
    "                   refit='AUC',\n",
    "                   return_train_score=True, \n",
    "                   verbose=1, n_jobs=-1)\n",
    "    \n",
    "    # Fit grid search\n",
    "    lrSearch.fit(X_train, y_train)\n",
    "    \n",
    "    # Store the results\n",
    "    results = lrSearch.cv_results_\n",
    "    \n",
    "    # View best hyperparameters\n",
    "    BEST_C =  lrSearch.best_estimator_.get_params()['C']\n",
    "    BEST_PENALTY = lrSearch.best_estimator_.get_params()['penalty']\n",
    "    BEST_SOLVER = lrSearch.best_estimator_.get_params()['solver']\n",
    "    \n",
    "    print('Best Penalty:', BEST_PENALTY)    \n",
    "    print('Best Solver:', BEST_SOLVER)\n",
    "\n",
    "    print('Best C:', BEST_C)\n",
    "    print('AUC', lrSearch.best_score_)\n",
    "    \n",
    "    cv_results = pd.DataFrame(results)\n",
    "    cv_results_subset =  cv_results.loc[:,['params','mean_test_Accuracy','mean_test_Precision', 'mean_test_Recall','mean_test_AUC', 'mean_test_F1']]\n",
    "    \n",
    "\n",
    "    return cv_results_subset, BEST_C, BEST_PENALTY, BEST_SOLVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TuneClassificationModel(X_train, y_train, estimator, param_grid, k=2): \n",
    "    # Stratified kfold as cross validation strategy\n",
    "    cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Use Multi scorer\n",
    "    scoring  = {'Accuracy':'accuracy','AUC':'roc_auc'}\n",
    "    \n",
    "    # Create grid search using StratifiedKFold cross validation\n",
    "    # Create grid search using StratifiedKFold cross validation\n",
    "    gridSearch = GridSearchCV(estimator = estimator, \n",
    "                   param_grid = param_grid, \n",
    "                   cv=cv, \n",
    "                   scoring=scoring ,\n",
    "                   refit='AUC',\n",
    "                   return_train_score=True, \n",
    "                   verbose=1, n_jobs=-1)\n",
    "    \n",
    "    # Fit grid search\n",
    "    gridSearch.fit(X_train, y_train)\n",
    "    \n",
    "    # Store the results\n",
    "    results = gridSearch.cv_results_\n",
    "    \n",
    "    # View best hyperparameters\n",
    "    print('Best Params:', gridSearch.best_params_)\n",
    "    print('AUC', gridSearch.best_score_)\n",
    "    \n",
    "    cv_results = pd.DataFrame(results)\n",
    "    return cv_results.loc[:,['params','mean_train_Accuracy','mean_test_Accuracy','mean_train_AUC', 'mean_test_AUC']]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression on Imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate logistic regression \n",
    "lr = LogisticRegression(random_state=123)\n",
    "  \n",
    "# Create regularization penalty space\n",
    "C = np.logspace(-4,4,50)\n",
    "     \n",
    "# Create params options\n",
    "param_grid = [\n",
    "      {'C' :C, 'penalty': ['l1'], 'solver': [ 'saga', 'liblinear']},\n",
    "      {'C':C, 'penalty': ['l2'], 'solver': ['newton-cg', 'lbfgs']},\n",
    "]\n",
    "\n",
    "TuneClassificationModel(X_train, y_train, lr, param_grid, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest Classifier on Imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "rf = RandomForestClassifier(random_state=123)\n",
    "\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = { \n",
    "    'max_depth': range(5, 25, 5), \n",
    "    'criterion': [\"entropy\", \"gini\"]\n",
    "}\n",
    "TuneClassificationModel(X_train, y_train, rf, param_grid, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "rf = RandomForestClassifier(criterion = 'entropy',\n",
    "                            max_depth = 10,\n",
    "                            random_state=123)\n",
    "\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = { \n",
    "     'n_estimators': range(5,100,10)\n",
    "}\n",
    "\n",
    "TuneClassificationModel(X_train, y_train, rf, param_grid, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "rf = RandomForestClassifier(criterion = 'entropy',\n",
    "                            max_depth = 10,\n",
    "                            n_estimators = 85,\n",
    "                            random_state=123)\n",
    "\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = { \n",
    "     'max_features': range(5,35,5)\n",
    "}\n",
    "\n",
    "TuneClassificationModel(X_train, y_train, rf, param_grid, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "rf = RandomForestClassifier(criterion = 'entropy',\n",
    "                            max_depth = 10,\n",
    "                            n_estimators = 85,\n",
    "                            max_features=5,\n",
    "                            random_state=123)\n",
    "\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = { \n",
    "     'min_samples_leaf': range(100,400,50)\n",
    "}\n",
    "\n",
    "TuneClassificationModel(X_train, y_train, rf, param_grid, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "rf = RandomForestClassifier(criterion = 'entropy',\n",
    "                            max_depth = 10,\n",
    "                            n_estimators = 85,\n",
    "                            max_features=5,\n",
    "                            min_samples_leaf=250,\n",
    "                            random_state=123)\n",
    "\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = { \n",
    "      'min_samples_split': range(200,500,50)\n",
    "}\n",
    "\n",
    "TuneClassificationModel(X_train, y_train, rf, param_grid, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = { \n",
    "      'n_neighbors':range(3,20,2),\n",
    "      'p':[1,2]\n",
    "}\n",
    "\n",
    "TuneClassificationModel(X_train, y_train, knn, param_grid, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn import linear_model #import the package\n",
    "\n",
    "num_C = ______  #--> list of values\n",
    "cv_num =   #--> list of values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### perfom cross validation on the X_train & y_train to create:\n",
    "- X_train_cv\n",
    "- X_test_cv \n",
    "- y_train_cv\n",
    "- y_test_cv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross validation\n",
    "\n",
    "#perform hyperparameter tuning\n",
    "\n",
    "#print the evaluation result by choosing a evaluation metric\n",
    "\n",
    "#print the optimum value of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarly explore other algorithms by building models like:\n",
    "- KNN\n",
    "- SVM\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proceed with the model which shows the best result \n",
    "- Apply the best hyperparameter on the model\n",
    "- Predict on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ___  #initialise the model with optimum hyperparameters\n",
    "clf.fit(X_train, y_train)\n",
    "print --> #print the evaluation score on the X_test by choosing the best evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the important features of the best model to understand the dataset\n",
    "- This will not give much explanation on the already transformed dataset\n",
    "- But it will help us in understanding if the dataset is not PCA transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp = []\n",
    "for i in clf.feature_importances_:\n",
    "    var_imp.append(i)\n",
    "print('Top var =', var_imp.index(np.sort(clf.feature_importances_)[-1])+1)\n",
    "print('2nd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-2])+1)\n",
    "print('3rd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-3])+1)\n",
    "\n",
    "# Variable on Index-16 and Index-13 seems to be the top 2 variables\n",
    "top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-1])\n",
    "second_top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-2])\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "np.random.shuffle(X_train_0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "\n",
    "plt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n",
    "            label='Actual Class-0 Examples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building with balancing Classes\n",
    "\n",
    "##### Perform class balancing with :\n",
    "- Random Oversampling\n",
    "- SMOTE\n",
    "- ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "- Build different models on the balanced dataset and see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn import linear_model #import the package\n",
    "\n",
    "num_C = ______  #--> list of values\n",
    "cv_num =   #--> list of values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### perfom cross validation on the X_train & y_train to create:\n",
    "- X_train_cv\n",
    "- X_test_cv \n",
    "- y_train_cv\n",
    "- y_test_cv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn import over_sampling #- import the packages\n",
    "\n",
    "#perform cross validation & then balance classes on X_train_cv & y_train_cv using Random Oversampling\n",
    "\n",
    "#perform hyperparameter tuning\n",
    "\n",
    "#print the evaluation result by choosing a evaluation metric\n",
    "\n",
    "#print the optimum value of hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarly explore other algorithms on balanced dataset by building models like:\n",
    "- KNN\n",
    "- SVM\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the class distribution after applying SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "sm = over_sampling.SMOTE(random_state=0)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n",
    "# Artificial minority samples and corresponding minority labels from SMOTE are appended\n",
    "# below X_train and y_train respectively\n",
    "# So to exclusively get the artificial minority samples from SMOTE, we do\n",
    "X_train_smote_1 = X_train_smote[X_train.shape[0]:]\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_smote_1[:X_train_1.shape[0], 0], X_train_smote_1[:X_train_1.shape[0], 1],\n",
    "            label='Artificial SMOTE Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross validation & then balance classes on X_train_cv & y_train_cv using SMOTE\n",
    "\n",
    "#perform hyperparameter tuning\n",
    "\n",
    "#print the evaluation result by choosing a evaluation metric\n",
    "\n",
    "#print the optimum value of hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build models on other algorithms to see the better performing on SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the class distribution after applying ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from imblearn import over_sampling\n",
    "\n",
    "ada = over_sampling.ADASYN(random_state=0)\n",
    "X_train_adasyn, y_train_adasyn = ada.fit_resample(X_train, y_train)\n",
    "# Artificial minority samples and corresponding minority labels from ADASYN are appended\n",
    "# below X_train and y_train respectively\n",
    "# So to exclusively get the artificial minority samples from ADASYN, we do\n",
    "X_train_adasyn_1 = X_train_adasyn[X_train.shape[0]:]\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_adasyn_1[:X_train_1.shape[0], 0], X_train_adasyn_1[:X_train_1.shape[0], 1],\n",
    "            label='Artificial ADASYN Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross validation & then balance classes on X_train_cv & y_train_cv using ADASYN\n",
    "\n",
    "#perform hyperparameter tuning\n",
    "\n",
    "#print the evaluation result by choosing a evaluation metric\n",
    "\n",
    "#print the optimum value of hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build models on other algorithms to see the better performing on ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the oversampling method which shows the best result on a model\n",
    "- Apply the best hyperparameter on the model\n",
    "- Predict on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the best oversampling method on X_train & y_train\n",
    "\n",
    "clf = ___  #initialise the model with optimum hyperparameters\n",
    "clf.fit( ) # fit on the balanced dataset\n",
    "print() --> #print the evaluation score on the X_test by choosing the best evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the important features of the best model to understand the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp = []\n",
    "for i in clf.feature_importances_:\n",
    "    var_imp.append(i)\n",
    "print('Top var =', var_imp.index(np.sort(clf.feature_importances_)[-1])+1)\n",
    "print('2nd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-2])+1)\n",
    "print('3rd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-3])+1)\n",
    "\n",
    "# Variable on Index-13 and Index-9 seems to be the top 2 variables\n",
    "top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-1])\n",
    "second_top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-2])\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "np.random.shuffle(X_train_0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "\n",
    "plt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n",
    "            label='Actual Class-0 Examples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Print the FPR,TPR & select the best threshold from the roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train auc =', metrics.roc_auc_score(_________)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(_________)\n",
    "threshold = thresholds[np.argmax(tpr-fpr)]\n",
    "print(threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
